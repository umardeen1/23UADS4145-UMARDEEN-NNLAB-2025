{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056fcce0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 784).astype(np.float32) / 255.0\n",
    "x_test = x_test.reshape(-1, 784).astype(np.float32) / 255.0\n",
    "y_train = tf.one_hot(y_train, depth=10)\n",
    "y_test = tf.one_hot(y_test, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d2c065",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(hidden1=128, hidden2=64, activation=tf.nn.relu, lr=0.01, epochs=10, batch_size=100):\n",
    "    input_size = 784\n",
    "    output_size = 10\n",
    "\n",
    "    W1 = tf.Variable(tf.random.normal([input_size, hidden1], stddev=0.1))\n",
    "    b1 = tf.Variable(tf.zeros([hidden1]))\n",
    "    W2 = tf.Variable(tf.random.normal([hidden1, hidden2], stddev=0.1))\n",
    "    b2 = tf.Variable(tf.zeros([hidden2]))\n",
    "    W3 = tf.Variable(tf.random.normal([hidden2, output_size], stddev=0.1))\n",
    "    b3 = tf.Variable(tf.zeros([output_size]))\n",
    "\n",
    "    def forward(x):\n",
    "        z1 = tf.matmul(x, W1) + b1\n",
    "        a1 = activation(z1)\n",
    "        z2 = tf.matmul(a1, W2) + b2\n",
    "        a2 = activation(z2)\n",
    "        return tf.matmul(a2, W3) + b3\n",
    "\n",
    "    def compute_loss(logits, labels):\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "    def compute_accuracy(logits, labels):\n",
    "        preds = tf.argmax(logits, axis=1)\n",
    "        actual = tf.argmax(labels, axis=1)\n",
    "        return tf.reduce_mean(tf.cast(tf.equal(preds, actual), tf.float32))\n",
    "\n",
    "    num_batches = x_train.shape[0] // batch_size\n",
    "    for epoch in range(epochs):\n",
    "        avg_loss = 0\n",
    "        for i in range(num_batches):\n",
    "            start, end = i * batch_size, (i + 1) * batch_size\n",
    "            x_batch = x_train[start:end]\n",
    "            y_batch = y_train[start:end]\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = forward(x_batch)\n",
    "                loss = compute_loss(logits, y_batch)\n",
    "            grads = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
    "            for var, grad in zip([W1, b1, W2, b2, W3, b3], grads):\n",
    "                var.assign_sub(lr * grad)\n",
    "            avg_loss += loss.numpy()\n",
    "\n",
    "        test_logits = forward(x_test)\n",
    "        test_acc = compute_accuracy(test_logits, y_test)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {test_acc.numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a847866",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Try different activation functions and hidden layer sizes\n",
    "train_model(hidden1=128, hidden2=64, activation=tf.nn.relu, lr=0.01, epochs=10, batch_size=100)\n",
    "train_model(hidden1=256, hidden2=128, activation=tf.nn.relu, lr=0.005, epochs=10, batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
