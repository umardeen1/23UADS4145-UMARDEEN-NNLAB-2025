{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_zWSFx4aXJI",
        "outputId": "93dd418c-bf94-4f39-9e1b-6915c802c41d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 784).astype(np.float32) / 255.0\n",
        "x_test = x_test.reshape(-1, 784).astype(np.float32) / 255.0\n",
        "y_train = tf.one_hot(y_train, depth=10)\n",
        "y_test = tf.one_hot(y_test, depth=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(hidden1=128, hidden2=64, activation=tf.nn.relu, lr=0.01, epochs=10, batch_size=100):\n",
        "    input_size = 784\n",
        "    output_size = 10\n",
        "\n",
        "    W1 = tf.Variable(tf.random.normal([input_size, hidden1], stddev=0.1))\n",
        "    b1 = tf.Variable(tf.zeros([hidden1]))\n",
        "    W2 = tf.Variable(tf.random.normal([hidden1, hidden2], stddev=0.1))\n",
        "    b2 = tf.Variable(tf.zeros([hidden2]))\n",
        "    W3 = tf.Variable(tf.random.normal([hidden2, output_size], stddev=0.1))\n",
        "    b3 = tf.Variable(tf.zeros([output_size]))\n",
        "\n",
        "    def forward(x):\n",
        "        z1 = tf.matmul(x, W1) + b1\n",
        "        a1 = activation(z1)\n",
        "        z2 = tf.matmul(a1, W2) + b2\n",
        "        a2 = activation(z2)\n",
        "        return tf.matmul(a2, W3) + b3\n",
        "\n",
        "    def compute_loss(logits, labels):\n",
        "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
        "\n",
        "    def compute_accuracy(logits, labels):\n",
        "        preds = tf.argmax(logits, axis=1)\n",
        "        actual = tf.argmax(labels, axis=1)\n",
        "        return tf.reduce_mean(tf.cast(tf.equal(preds, actual), tf.float32))\n",
        "\n",
        "    num_batches = x_train.shape[0] // batch_size\n",
        "    for epoch in range(epochs):\n",
        "        avg_loss = 0\n",
        "        for i in range(num_batches):\n",
        "            start, end = i * batch_size, (i + 1) * batch_size\n",
        "            x_batch = x_train[start:end]\n",
        "            y_batch = y_train[start:end]\n",
        "            with tf.GradientTape() as tape:\n",
        "                logits = forward(x_batch)\n",
        "                loss = compute_loss(logits, y_batch)\n",
        "            grads = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
        "            for var, grad in zip([W1, b1, W2, b2, W3, b3], grads):\n",
        "                var.assign_sub(lr * grad)\n",
        "            avg_loss += loss.numpy()\n",
        "\n",
        "        test_logits = forward(x_test)\n",
        "        test_acc = compute_accuracy(test_logits, y_test)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {test_acc.numpy():.4f}\")\n"
      ],
      "metadata": {
        "id": "Jzhxl7Pgahb8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try different activation functions and hidden layer sizes\n",
        "train_model(hidden1=128, hidden2=64, activation=tf.nn.relu, lr=0.01, epochs=10, batch_size=100)\n",
        "train_model(hidden1=256, hidden2=128, activation=tf.nn.relu, lr=0.005, epochs=10, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVcm016Jakz0",
        "outputId": "7432d2a9-8829-45eb-cb45-f3289440ea87"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 769.3507, Accuracy: 0.8203\n",
            "Epoch 2/10, Loss: 314.3485, Accuracy: 0.8794\n",
            "Epoch 3/10, Loss: 241.1804, Accuracy: 0.8977\n",
            "Epoch 4/10, Loss: 210.6573, Accuracy: 0.9068\n",
            "Epoch 5/10, Loss: 192.2643, Accuracy: 0.9132\n",
            "Epoch 6/10, Loss: 178.9829, Accuracy: 0.9187\n",
            "Epoch 7/10, Loss: 168.4487, Accuracy: 0.9225\n",
            "Epoch 8/10, Loss: 159.6407, Accuracy: 0.9256\n",
            "Epoch 9/10, Loss: 152.0442, Accuracy: 0.9288\n",
            "Epoch 10/10, Loss: 145.3033, Accuracy: 0.9308\n",
            "Epoch 1/10, Loss: 930.0171, Accuracy: 0.8579\n",
            "Epoch 2/10, Loss: 421.8740, Accuracy: 0.8918\n",
            "Epoch 3/10, Loss: 340.5153, Accuracy: 0.9061\n",
            "Epoch 4/10, Loss: 301.7886, Accuracy: 0.9122\n",
            "Epoch 5/10, Loss: 276.7086, Accuracy: 0.9176\n",
            "Epoch 6/10, Loss: 258.0137, Accuracy: 0.9226\n",
            "Epoch 7/10, Loss: 242.9494, Accuracy: 0.9263\n",
            "Epoch 8/10, Loss: 230.2112, Accuracy: 0.9303\n",
            "Epoch 9/10, Loss: 219.1077, Accuracy: 0.9329\n",
            "Epoch 10/10, Loss: 209.2189, Accuracy: 0.9358\n"
          ]
        }
      ]
    }
  ]
}